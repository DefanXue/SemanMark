"""
ç¼–ç å™¨åŠ è½½ä¸åµŒå…¥æ¥å£ï¼ˆç¬¬ä¸€é˜¶æ®µéª¨æ¶ï¼‰ï¼š
- ä¸¥æ ¼å¤ç”¨ contrastive_learning/scripts/evaluate.py çš„åŠ è½½ä¸æ¨ç†é€»è¾‘ï¼?  - æ£€æµ?PEFT é€‚é…å™¨ç›®å½•ï¼ˆadapter_config.jsonï¼?  - ä½¿ç”¨åŸºç¡€æ¨¡å‹ tokenizer
  - æ„å»º RobustEncoder(eval_mode=True) å¹¶æ³¨å…?PEFT é€‚é…å™?  - ä»…ä½¿ç”?encoder-only çš„è¡¨ç¤ºï¼ˆforward_encoder_onlyï¼?- é»˜è®¤å¼€å¯é‡åŒ–ï¼ˆä¸è¯„æµ‹é»˜è®¤ä¸€è‡´ï¼‰ï¼Œmax_length=512
- æ‰¹é‡åµŒå…¥æ¥å£ embed_codes

æ³¨æ„ï¼šæœ¬æ¨¡å—ä»…åœ¨ D:\kyl410\XDF\Watermark4code å†…å®ç°ï¼Œä¸ä¿®æ”¹å…¶ä»–ç›®å½•ã€?"""

import os
import sys
from typing import List, Tuple, Optional

import torch
import numpy as np
from transformers import AutoTokenizer


def _ensure_sys_path_for_contrastive_learning() -> None:
    """
    å°?SrcMarker-main åŠ å…¥ sys.pathï¼Œç¡®ä¿å¯ä»¥å¯¼å…?contrastive_learning åŒ…ã€?    è·¯å¾„æ¨å¯¼ï¼šWatermark4code ä½äº XDF/Watermark4codeï¼Œå…„å¼Ÿç›®å½•ä¸º SrcMarker-mainã€?    """
    current_dir = os.path.dirname(os.path.abspath(__file__))
    xdf_root = os.path.abspath(os.path.join(current_dir, os.pardir, os.pardir))
    srcmarker_root = os.path.join(xdf_root, "SrcMarker-main")
    # ç¡®ä¿ä¼˜å…ˆå‘½ä¸­ XDF/SrcMarker-main ä¸‹çš„ contrastive_learning
    if srcmarker_root in sys.path:
        sys.path.remove(srcmarker_root)
    sys.path.insert(0, srcmarker_root)

_ensure_sys_path_for_contrastive_learning()

from contrastive_learning.model import RobustEncoder  # noqa: E402
from peft import PeftConfig, PeftModel  # noqa: E402

# ä¸è¯„æµ‹è„šæœ¬ä¸€è‡´ï¼šä»…åœ¨æœªé¢„è®¾æ—¶ä¸?HF ç¼“å­˜ä¸ç¦»çº¿æ ‡å¿—æä¾›é»˜è®¤å€?_CUR_DIR = os.path.dirname(os.path.abspath(__file__))
_XDF_ROOT = os.path.abspath(os.path.join(_CUR_DIR, os.pardir, os.pardir))
_HF_CACHE_DEFAULT = os.path.join(_XDF_ROOT, "SrcMarker-main", "hf-cache")
os.environ.setdefault("HF_HOME", _HF_CACHE_DEFAULT)
os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")
os.environ.setdefault("HF_HUB_OFFLINE", "1")


def load_best_model(
    model_dir: str,
    use_quantization: bool = True,
    device: Optional[torch.device] = None,
) -> Tuple[RobustEncoder, AutoTokenizer]:
    """
    åŠ è½½é²æ£’ç¼–ç å™¨ï¼ˆä¸?contrastive_learning/scripts/evaluate.py çš„åŠ è½½æµç¨‹å¯¹é½ï¼‰ï¼?    - è‹¥æ£€æµ‹åˆ° PEFT é€‚é…å™¨ï¼ˆadapter_config.jsonï¼‰ï¼š
      * ä»é€‚é…å™¨é…ç½®è¯»å?base_model_name
      * ä½¿ç”¨è¯?base tokenizerï¼ˆä¸æŒ‡å®š cache_dir/local_files_onlyï¼?      * æ„å»º RobustEncoder(eval_mode=True, use_quantization)
      * æ³¨å…¥ PeftModel.from_pretrained(model.encoder, model_dir)
    - å¦åˆ™å°è¯•åŠ è½½å®Œæ•´æ¨¡å‹ï¼ˆä» model_dir/pytorch_model.bin å›é€€ï¼‰ï¼Œ
      * tokenizer ä»é…ç½®æˆ–é»˜è®¤æ¨¡å‹ååŠ è½½ï¼ˆä¸è¯„æµ‹è¡Œä¸ºä¸€è‡´ï¼Œä¸å¼ºåˆ?cache_dirï¼?
    è¿”å›ï¼?model, tokenizer)
    """
    device = device or (torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu"))

    adapter_cfg_path = os.path.join(model_dir, "adapter_config.json")
    is_peft = os.path.exists(adapter_cfg_path)

    if is_peft:
        # ä¸è¯„æµ‹è„šæœ¬ä¸€è‡´ï¼šä½¿ç”¨ä¼ å…¥çš?model_dir ä½œä¸ºé€‚é…å™¨ç›®å½?        peft_config = PeftConfig.from_pretrained(model_dir)
        base_model_name = peft_config.base_model_name_or_path
        
        # æ„å»ºæœ¬åœ°å¿«ç…§è·¯å¾„ï¼Œé¿å…å°è¯•ä»è¿œç¨‹åŠ è½½
        # base_model_name æ ¼å¼: "Salesforce/codet5-base"
        model_name_safe = base_model_name.replace("/", "--")  # "Salesforce--codet5-base"
        local_model_path = os.path.join(
            _HF_CACHE_DEFAULT,
            "hub",
            f"models--{model_name_safe}",
            "snapshots"
        )
        
        # æŸ¥æ‰¾æœ€æ–°çš„å¿«ç…§ç›®å½•
        if os.path.exists(local_model_path):
            snapshots = [d for d in os.listdir(local_model_path) if os.path.isdir(os.path.join(local_model_path, d))]
            if snapshots:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¿«ç…§ï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰
                snapshot_path = os.path.join(local_model_path, snapshots[0])
                tokenizer = AutoTokenizer.from_pretrained(snapshot_path)
            else:
                # å›é€€åˆ°è¿œç¨‹åç§°ï¼ˆå¯èƒ½è§¦å‘ä¸‹è½½ï¼?                tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        else:
            # å›é€€åˆ°è¿œç¨‹åç§?            tokenizer = AutoTokenizer.from_pretrained(base_model_name)

        model = RobustEncoder(
            model_name=base_model_name,
            projection_dim=128,
            projection_hidden_dim=512,
            pooling_strategy="mean",
            use_quantization=use_quantization,
            eval_mode=True,
        )
        model.encoder = PeftModel.from_pretrained(model.encoder, model_dir)
    else:
        # é?PEFTï¼šä¸è¯„æµ‹è„šæœ¬ç›¸åŒç­–ç•¥ï¼Œå…ˆæŒ‰é»˜è®¤åŸºç¡€æ¨¡å‹ååŠ è½?tokenizerï¼Œå†ä»æœ¬åœ°æƒé‡å›é€€
        base_model_name = "Salesforce/codet5-base"
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        model = RobustEncoder(
            model_name=base_model_name,
            projection_dim=128,
            projection_hidden_dim=512,
            pooling_strategy="mean",
            use_quantization=use_quantization,
            eval_mode=True,
        )
        pt_path = os.path.join(model_dir, "pytorch_model.bin")
        if os.path.exists(pt_path):
            state_dict = torch.load(pt_path, map_location=device)
            filtered = {k: v for k, v in state_dict.items() if not k.startswith("projection_head.")}
            model.load_state_dict(filtered, strict=False)

    model.to(device)
    model.eval()
    return model, tokenizer


@torch.no_grad()
def embed_codes(
    model: RobustEncoder,
    tokenizer: AutoTokenizer,
    code_list: List[str],
    max_length: int = 512,
    batch_size: int = 32,
    device: Optional[torch.device] = None,
) -> np.ndarray:
    """
    å°†ä¸€ç»„ä»£ç å­—ç¬¦ä¸²ç¼–ç ä¸?L2 å½’ä¸€åŒ–çš„ 768 ç»´å‘é‡ï¼ˆencoder-only è¡¨ç¤ºï¼‰ã€?
    å‚æ•°ï¼?    - model / tokenizerï¼šç”± load_best_model è¿”å›ï¼Œæ¨¡å‹å·²å¤„äº eval æ¨¡å¼
    - code_listï¼šä»£ç å­—ç¬¦ä¸²åˆ—è¡¨
    - max_lengthï¼šåˆ†è¯å™¨æœ€å¤?token é•¿åº¦ï¼ˆä¸è¯„æµ‹ä¸€è‡´ä¸º 512ï¼?    - batch_sizeï¼šæ‰¹å¤§å°
    - deviceï¼šå¯é€‰è®¾å¤‡ï¼Œé»˜è®¤ä¸?model ä¿æŒä¸€è‡?
    è¿”å›ï¼?    - numpy.ndarrayï¼Œå½¢çŠ?[N, 768]
    """
    if not code_list:
        return np.zeros((0, 768), dtype=np.float32)

    device = device or next(model.parameters()).device
    all_embeddings: List[np.ndarray] = []

    total = len(code_list)
    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        batch_codes = code_list[start:end]

        encodings = tokenizer(
            batch_codes,
            max_length=max_length,
            padding=True,
            truncation=True,
            return_tensors="pt",
        )
        input_ids = encodings["input_ids"].to(device)
        attention_mask = encodings["attention_mask"].to(device)

        outputs = model.forward_encoder_only(input_ids=input_ids, attention_mask=attention_mask)
        embeddings = outputs.detach().cpu().numpy()  # [B, 768]
        all_embeddings.append(embeddings.astype(np.float32))

    return np.concatenate(all_embeddings, axis=0)


