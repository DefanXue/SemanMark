"""
é¡¶å±‚ API å°è£…ï¼ˆç¬¬ä¸€é˜¶æ®µæœ€å°é—­ç¯ï¼‰ï¼?- åŠ è½½æ¨¡å‹ï¼ˆé»˜è®¤é‡åŒ–ï¼‰ã€æ‰¹é‡åµŒå…?- ä»å›ºå®šç§é’¥æ´¾ç”?4 ä¸ªæ–¹å?- åŸºçº¿/æ£€æµ‹æŠ•å½±ä¸åˆ¤å®š

ä¸¥æ ¼å¤ç”¨ contrastive_learning è¯„æµ‹è·¯å¾„ï¼šencoder-onlyã€mean æ± åŒ–ã€L2 å½’ä¸€åŒ–ã€max_length=512ã€?"""

from typing import Dict, List, Tuple
import os
import numpy as np
import torch

try:
    # åŒ…å†…ç›¸å¯¹å¯¼å…¥ï¼ˆä½œä¸ºåŒ…è¿è¡Œæ—¶ï¼‰
    from .encoder import load_best_model, embed_codes
    from .keys import derive_directions
    from .utils import project_embeddings, compute_thresholds, detect_bits
except Exception:
    # ç›´æ¥è„šæœ¬å¯¼å…¥çš„å›é€€ï¼ˆåœ¨ Watermark4code ç›®å½•å†…è¿è¡?python test.py æ—¶ï¼‰
    import os, sys
    _CUR_DIR = os.path.dirname(os.path.abspath(__file__))
    if _CUR_DIR not in sys.path:
        sys.path.append(_CUR_DIR)
    from encoder import load_best_model, embed_codes  # type: ignore
    from keys.directions import derive_directions  # type: ignore
    from utils.math import project_embeddings, compute_thresholds, detect_bits  # type: ignore


DEFAULT_SECRET = "WATERMARK_SECRET"
DEFAULT_D = 768
DEFAULT_K = 4
DEFAULT_T_MARGIN = 0.10


def load_encoder(model_dir: str, use_quantization: bool = True):
    """
    åŒ…è£…åº•å±‚åŠ è½½ï¼Œè¿”å›?(model, tokenizer, device)
    """
    model, tokenizer = load_best_model(model_dir=model_dir, use_quantization=use_quantization)
    device = next(model.parameters()).device
    return model, tokenizer, device


def compute_baseline_s0(
    model_dir: str,
    codes: List[str],
    secret_key: str = DEFAULT_SECRET,
    use_quantization: bool = True,
    max_length: int = 512,
    batch_size: int = 32,
) -> Dict:
    """
    è®¡ç®—ç»™å®šä»£ç é›†åˆçš„åŸºçº¿æŠ•å½±ï¼š
    - è¿”å›åŒ…å« embeddings, directions, s0 çš„å­—å…¸ï¼ˆå‡ä¸º numpy æ•°ç»„ï¼?    """
    model, tokenizer, device = load_encoder(model_dir, use_quantization)
    embs = embed_codes(model, tokenizer, codes, max_length=max_length, batch_size=batch_size, device=device)
    dirs = derive_directions(secret_key=secret_key, d=embs.shape[1], k=DEFAULT_K)
    s0 = project_embeddings(embs, dirs)  # [N,K]
    return {
        "embeddings": embs,
        "directions": dirs,
        "s0": s0,
    }


def detect_bits_for_codes(
    model_dir: str,
    codes: List[str],
    directions: np.ndarray,
    s0: np.ndarray,
    use_quantization: bool = True,
    t_margin: float = DEFAULT_T_MARGIN,
    max_length: int = 512,
    batch_size: int = 32,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¯¹ä»£ç è®¡ç®—æŠ•å½±ä¸æ¯”ç‰¹åˆ¤å®šï¼?    - è¿”å› (projections, bits)ï¼Œå½¢çŠ¶å‡ä¸?[N,K]
    """
    model, tokenizer, device = load_encoder(model_dir, use_quantization)
    embs = embed_codes(model, tokenizer, codes, max_length=max_length, batch_size=batch_size, device=device)
    projections = project_embeddings(embs, directions)
    bits = detect_bits(projections, s0, t_margin=t_margin)
    return projections, bits


